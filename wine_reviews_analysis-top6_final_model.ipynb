{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import csv\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import model_evaluation_utils as meu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset (already have isolated the two columns for this analysis)\n",
    "wine = pd.read_csv('winemag_filt.csv')\n",
    "\n",
    "# Filter for 6 most popular wine varieties by frequency in the dataset excluding blends\n",
    "top6 = ['Pinot Noir','Chardonnay','Cabernet Sauvignon','Riesling','Sauvignon Blanc','Syrah']\n",
    "\n",
    "wine = wine[wine.variety.isin(top6)]\n",
    "\n",
    "# Take a random sample of 10000 observations from that filtered set\n",
    "#wine_samp = wine.sample(n=10000, random_state=27)\n",
    "\n",
    "# Format columns as lists\n",
    "corpus = wine['description'].values.tolist()\n",
    "labels = wine['variety'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accented char function\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special char function\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)\n",
    "\n",
    "# Lemmatization function (version of stemming that maintains English spellings)\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer function\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the functions together now!\n",
    "def normalize_corpus(corpus,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying to the data\n",
    "norm_corpus = normalize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making training and test sets\n",
    "train_corpus, test_corpus, train_label_names, test_label_names = train_test_split(norm_corpus, labels, test_size=0.3, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of distribution of varities in test, training datasets\n",
    "trd = dict(Counter(train_label_names))\n",
    "tsd = dict(Counter(test_label_names))\n",
    "\n",
    "(pd.DataFrame([[key, trd[key], tsd[key]] for key in trd], \n",
    "             columns=['Target Label', 'Train Count', 'Test Count'])\n",
    ".sort_values(by=['Train Count', 'Test Count'],\n",
    "             ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Bag of Words features on train articles\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0)\n",
    "cv_train_features = cv.fit_transform(train_corpus)\n",
    "\n",
    "# transform test articles into features\n",
    "cv_test_features = cv.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes model!\n",
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb.fit(cv_train_features, train_label_names)\n",
    "mnb_bow_cv_scores = cross_val_score(mnb, cv_train_features, train_label_names, cv=5)\n",
    "mnb_bow_cv_mean_score = np.mean(mnb_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', mnb_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', mnb_bow_cv_mean_score)\n",
    "mnb_bow_test_score = mnb.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', mnb_bow_test_score)\n",
    "\n",
    "# CV Accuracy (5-fold): [0.83947908 0.83635831 0.8374817  0.84333821 0.84070278]\n",
    "# Mean CV Accuracy: 0.8394720166053153\n",
    "# Test Accuracy: 0.8512876562606736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression model!\n",
    "lr = LogisticRegression(penalty='l2', max_iter=100, C=1, random_state=27)\n",
    "lr.fit(cv_train_features, train_label_names)\n",
    "lr_bow_cv_scores = cross_val_score(lr, cv_train_features, train_label_names, cv=5)\n",
    "lr_bow_cv_mean_score = np.mean(lr_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', lr_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', lr_bow_cv_mean_score)\n",
    "lr_bow_test_score = lr.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', lr_bow_test_score)\n",
    "\n",
    "# CV Accuracy (5-fold): [0.86391572 0.86372951 0.86032211 0.86486091 0.86412884]\n",
    "# Mean CV Accuracy: 0.8633914166360599\n",
    "# Test Accuracy: 0.8708928205478517"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearSVC model!\n",
    "svm = LinearSVC(penalty='l2', C=1, random_state=27)\n",
    "svm.fit(cv_train_features, train_label_names)\n",
    "svm_bow_cv_scores = cross_val_score(svm, cv_train_features, train_label_names, cv=5)\n",
    "svm_bow_cv_mean_score = np.mean(svm_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svm_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', svm_bow_cv_mean_score)\n",
    "svm_bow_test_score = svm.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svm_bow_test_score)\n",
    "\n",
    "# CV Accuracy (5-fold): [0.84108867 0.84279859 0.83426061 0.83806735 0.84070278]\n",
    "# Mean CV Accuracy: 0.8393836031658332\n",
    "# Test Accuracy: 0.8488284718901564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDGClassifier!\n",
    "svm_sgd = SGDClassifier(loss='hinge', penalty='l2', max_iter=5, random_state=27)\n",
    "svm_sgd.fit(cv_train_features, train_label_names)\n",
    "svmsgd_bow_cv_scores = cross_val_score(svm_sgd, cv_train_features, train_label_names, cv=5)\n",
    "svmsgd_bow_cv_mean_score = np.mean(svmsgd_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svmsgd_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', svmsgd_bow_cv_mean_score)\n",
    "svmsgd_bow_test_score = svm_sgd.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svmsgd_bow_test_score)\n",
    "\n",
    "# CV Accuracy (5-fold): [0.8444542  0.84909251 0.83762811 0.84392387 0.84670571]\n",
    "# Mean CV Accuracy: 0.8443608784243025\n",
    "# Test Accuracy: 0.8525855591228909"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rfc = RandomForestClassifier(n_estimators=10, random_state=27)\n",
    "rfc.fit(cv_train_features, train_label_names)\n",
    "rfc_bow_cv_scores = cross_val_score(rfc, cv_train_features, train_label_names, cv=5)\n",
    "rfc_bow_cv_mean_score = np.mean(rfc_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', rfc_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', rfc_bow_cv_mean_score)\n",
    "rfc_bow_test_score = rfc.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', rfc_bow_test_score)\n",
    "\n",
    "# CV Accuracy (5-fold): [0.79777583 0.79844848 0.79970717 0.79809663 0.79106881]\n",
    "# Mean CV Accuracy: 0.7970193850581991\n",
    "# Test Accuracy: 0.8059976774369834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "gbc = GradientBoostingClassifier(n_estimators=10, random_state=27)\n",
    "gbc.fit(cv_train_features, train_label_names)\n",
    "gbc_bow_cv_scores = cross_val_score(gbc, cv_train_features, train_label_names, cv=5)\n",
    "gbc_bow_cv_mean_score = np.mean(gbc_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', gbc_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', gbc_bow_cv_mean_score)\n",
    "gbc_bow_test_score = gbc.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', gbc_bow_test_score)\n",
    "\n",
    "# CV Accuracy (5-fold): [0.74422008 0.7441452  0.74114202 0.73923865 0.75065886]\n",
    "# Mean CV Accuracy: 0.7438809613264279\n",
    "# Test Accuracy: 0.7405560489104447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build BOW features on train articles with TFIDF\n",
    "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0)\n",
    "tv_train_features = tv.fit_transform(train_corpus)\n",
    "\n",
    "# transform test articles into features\n",
    "tv_test_features = tv.transform(test_corpus)\n",
    "\n",
    "# Now I'm going to re-run all of those models with this new set of features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb.fit(tv_train_features, train_label_names)\n",
    "mnb_tfidf_cv_scores = cross_val_score(mnb, tv_train_features, train_label_names, cv=5)\n",
    "mnb_tfidf_cv_mean_score = np.mean(mnb_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', mnb_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', mnb_tfidf_cv_mean_score)\n",
    "mnb_tfidf_test_score = mnb.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', mnb_tfidf_test_score)\n",
    "\n",
    "# CV Accuracy (5-fold): [0.74261048 0.74107143 0.73967789 0.74450952 0.74860908]\n",
    "# Mean CV Accuracy: 0.7432956783377672\n",
    "# Test Accuracy: 0.7549012910717945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2', max_iter=100, C=1, random_state=27)\n",
    "lr.fit(tv_train_features, train_label_names)\n",
    "lr_tfidf_cv_scores = cross_val_score(lr, tv_train_features, train_label_names, cv=5)\n",
    "lr_tfidf_cv_mean_score = np.mean(lr_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', lr_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', lr_tfidf_cv_mean_score)\n",
    "lr_tfidf_test_score = lr.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', lr_tfidf_test_score)\n",
    "\n",
    "# CV Accuracy (5-fold): [0.86289143 0.86080211 0.86325037 0.86325037 0.86661786]\n",
    "# Mean CV Accuracy: 0.8633624254782909\n",
    "# Test Accuracy: 0.8665209372224879"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(penalty='l2', C=1, random_state=27)\n",
    "svm.fit(tv_train_features, train_label_names)\n",
    "svm_tfidf_cv_scores = cross_val_score(svm, tv_train_features, train_label_names, cv=5)\n",
    "svm_tfidf_cv_mean_score = np.mean(svm_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svm_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', svm_tfidf_cv_mean_score)\n",
    "svm_tfidf_test_score = svm.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svm_tfidf_test_score)\n",
    "\n",
    "# CV Accuracy (5-fold): [0.8627451  0.86299766 0.86105417 0.86032211 0.86661786]\n",
    "# Mean CV Accuracy: 0.8627473799206935\n",
    "# Test Accuracy: 0.8703463351321812"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_sgd = SGDClassifier(loss='hinge', penalty='l2', max_iter=5, random_state=27)\n",
    "svm_sgd.fit(tv_train_features, train_label_names)\n",
    "svmsgd_tfidf_cv_scores = cross_val_score(svm_sgd, tv_train_features, train_label_names, cv=5)\n",
    "svmsgd_tfidf_cv_mean_score = np.mean(svmsgd_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svmsgd_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', svmsgd_tfidf_cv_mean_score)\n",
    "svmsgd_tfidf_test_score = svm_sgd.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svmsgd_tfidf_test_score)\n",
    "\n",
    "# CV Accuracy (5-fold): [0.85923325 0.8588993  0.86002928 0.86046852 0.86647145]\n",
    "# Mean CV Accuracy: 0.8610203592510397\n",
    "# Test Accuracy: 0.8620124325432065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=10, random_state=27)\n",
    "rfc.fit(tv_train_features, train_label_names)\n",
    "rfc_tfidf_cv_scores = cross_val_score(rfc, tv_train_features, train_label_names, cv=5)\n",
    "rfc_tfidf_cv_mean_score = np.mean(rfc_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', rfc_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', rfc_tfidf_cv_mean_score)\n",
    "rfc_tfidf_test_score = rfc.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', rfc_tfidf_test_score)\n",
    "\n",
    "# CV Accuracy (5-fold): [0.79323968 0.7876171  0.79399707 0.79165447 0.79414348]\n",
    "# Mean CV Accuracy: 0.7921303603827885\n",
    "# Test Accuracy: 0.8066124735296126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=10, random_state=27 )\n",
    "gbc.fit(tv_train_features, train_label_names)\n",
    "gbc_tfidf_cv_scores = cross_val_score(gbc, tv_train_features, train_label_names, cv=5)\n",
    "gbc_tfidf_cv_mean_score = np.mean(gbc_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', gbc_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', gbc_tfidf_cv_mean_score)\n",
    "gbc_tfidf_test_score = gbc.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', gbc_tfidf_test_score)\n",
    "\n",
    "# CV Accuracy (5-fold): [0.74539069 0.74399883 0.74040996 0.73923865 0.75021962]\n",
    "# Mean CV Accuracy: 0.7438515502069564\n",
    "# Test Accuracy: 0.7415807090648269"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a pretty table of all the results\n",
    "pd.DataFrame([['Naive Bayes', mnb_bow_cv_mean_score, mnb_bow_test_score, \n",
    "               mnb_tfidf_cv_mean_score, mnb_tfidf_test_score],\n",
    "              ['Logistic Regression', lr_bow_cv_mean_score, lr_bow_test_score, \n",
    "               lr_tfidf_cv_mean_score, lr_tfidf_test_score],\n",
    "              ['Linear SVM', svm_bow_cv_mean_score, svm_bow_test_score, \n",
    "               svm_tfidf_cv_mean_score, svm_tfidf_test_score],\n",
    "              ['Linear SVM (SGD)', svmsgd_bow_cv_mean_score, svmsgd_bow_test_score, \n",
    "               svmsgd_tfidf_cv_mean_score, svmsgd_tfidf_test_score],\n",
    "              ['Random Forest', rfc_bow_cv_mean_score, rfc_bow_test_score, \n",
    "               rfc_tfidf_cv_mean_score, rfc_tfidf_test_score],\n",
    "              ['Gradient Boosted Machines', gbc_bow_cv_mean_score, gbc_bow_test_score, \n",
    "               gbc_tfidf_cv_mean_score, gbc_tfidf_test_score]],\n",
    "             columns=['Model', 'CV Score (TF)', 'Test Score (TF)', 'CV Score (TF-IDF)', 'Test Score (TF-IDF)'],\n",
    "             ).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 1) ..........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 1) ..........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 1), total=   1.0s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 1) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 1) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 1), total=   1.0s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 1) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 1), total=   1.0s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 2) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 2), total=   2.7s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 2) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 2), total=   2.7s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 2) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 2), total=   2.9s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 2) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 2), total=   2.9s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 2) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 2), total=   3.1s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 2), total=   2.7s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 2), total=   2.8s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 2), total=   3.2s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 2), total=   3.1s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 2), total=   3.0s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 1), total=   1.4s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 1), total=   1.2s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 1), total=   1.2s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 2), total=   3.3s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 2), total=   3.4s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 2), total=   3.3s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 2), total=   3.3s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 2), total=   3.1s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 1), total=   1.6s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 1), total=   1.7s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 1), total=   1.7s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 1), total=   1.8s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 1), total=   1.7s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 2), total=   5.0s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 2), total=   5.5s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 2), total=   5.6s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 2), total=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  2.2min finished\n"
     ]
    }
   ],
   "source": [
    "# Fiddling with parameters of LinearSVM on TFIDF\n",
    "svm_pipeline = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                        ('svm', LinearSVC(random_state=27))\n",
    "                       ])\n",
    "\n",
    "param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "              'svm__C': [0.01, 0.1, 1, 5]\n",
    "}\n",
    "\n",
    "gs_svm = GridSearchCV(svm_pipeline, param_grid, cv=5, verbose=2)\n",
    "gs_svm = gs_svm.fit(train_corpus, train_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tfidf',\n",
       "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None)),\n",
       "  ('svm', LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "        intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "        multi_class='ovr', penalty='l2', random_state=27, tol=0.0001,\n",
       "        verbose=0))],\n",
       " 'tfidf': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       " 'svm': LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "      intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "      multi_class='ovr', penalty='l2', random_state=27, tol=0.0001,\n",
       "      verbose=0),\n",
       " 'tfidf__analyzer': 'word',\n",
       " 'tfidf__binary': False,\n",
       " 'tfidf__decode_error': 'strict',\n",
       " 'tfidf__dtype': numpy.float64,\n",
       " 'tfidf__encoding': 'utf-8',\n",
       " 'tfidf__input': 'content',\n",
       " 'tfidf__lowercase': True,\n",
       " 'tfidf__max_df': 1.0,\n",
       " 'tfidf__max_features': None,\n",
       " 'tfidf__min_df': 1,\n",
       " 'tfidf__ngram_range': (1, 2),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__preprocessor': None,\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__stop_words': None,\n",
       " 'tfidf__strip_accents': None,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tfidf__tokenizer': None,\n",
       " 'tfidf__use_idf': True,\n",
       " 'tfidf__vocabulary': None,\n",
       " 'svm__C': 1,\n",
       " 'svm__class_weight': None,\n",
       " 'svm__dual': True,\n",
       " 'svm__fit_intercept': True,\n",
       " 'svm__intercept_scaling': 1,\n",
       " 'svm__loss': 'squared_hinge',\n",
       " 'svm__max_iter': 1000,\n",
       " 'svm__multi_class': 'ovr',\n",
       " 'svm__penalty': 'l2',\n",
       " 'svm__random_state': 27,\n",
       " 'svm__tol': 0.0001,\n",
       " 'svm__verbose': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The best model!\n",
    "gs_svm.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8825\n",
      "Precision: 0.8827\n",
      "Recall: 0.8825\n",
      "F1 Score: 0.8808\n"
     ]
    }
   ],
   "source": [
    "svm_predictions = gs_svm.predict(test_corpus)\n",
    "unique_classes = list(set(test_label_names))\n",
    "meu.get_metrics(true_labels=test_label_names, predicted_labels=svm_predictions)\n",
    "\n",
    "# Accuracy: 0.8825\n",
    "# Precision: 0.8827\n",
    "# Recall: 0.8825\n",
    "# F1 Score: 0.8808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiddling with parameters of Logistic on TF\n",
    "lr_pipeline = Pipeline([('tf', CountVectorizer()),\n",
    "                        ('lr', LogisticRegression(random_state=27))\n",
    "                       ])\n",
    "\n",
    "param_grid = {'tf__ngram_range': [(1, 1), (1, 2)],\n",
    "              'lr__C': [0.01, 0.1, 1, 5]\n",
    "}\n",
    "\n",
    "gs_lr = GridSearchCV(lr_pipeline, param_grid, cv=5, verbose=2)\n",
    "gs_lr = gs_lr.fit(train_corpus, train_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictions = gs_lr.predict(test_corpus)\n",
    "unique_classes = list(set(test_label_names))\n",
    "meu.get_metrics(true_labels=test_label_names, predicted_labels=lr_predictions)\n",
    "\n",
    "# Accuracy: 0.8812\n",
    "# Precision: 0.8806\n",
    "# Recall: 0.8812\n",
    "# F1 Score: 0.8797"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meu.display_classification_report(true_labels=test_label_names, \n",
    "                                  predicted_labels=svm_predictions, classes=unique_classes)\n",
    "\n",
    "#    precision    recall  f1-score   support\n",
    "#\n",
    "#        Pinot Noir       0.89      0.94      0.91      3957\n",
    "#          Riesling       0.93      0.90      0.92      1563\n",
    "#   Sauvignon Blanc       0.87      0.76      0.81      1495\n",
    "#        Chardonnay       0.87      0.94      0.91      3496\n",
    "#             Syrah       0.88      0.70      0.78      1257\n",
    "# Cabernet Sauvignon       0.86      0.87      0.87      2871\n",
    "\n",
    "#         micro avg       0.88      0.88      0.88     14639\n",
    "#         macro avg       0.89      0.85      0.87     14639\n",
    "#      weighted avg       0.88      0.88      0.88     14639"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "unique_classes = list(set(test_label_names))\n",
    "\n",
    "cm_frame = pd.DataFrame(data=cm, \n",
    "                        columns=unique_classes, \n",
    "                        index=unique_classes )\n",
    "print(cm_frame) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Review              Actual  \\\n",
      "0      big toasty brood first calm become glass pleas...               Syrah   \n",
      "1      start decently menthol toast aroma new oak lay...          Chardonnay   \n",
      "2      simple pinot noir little sweet spritzy taste l...          Pinot Noir   \n",
      "3      ripe wine violet aroma layer ripe jammy berry ...               Syrah   \n",
      "4      source three acre vineyard plant smooth quite ...          Pinot Noir   \n",
      "5      touch petrol lend slick mineral sheen luscious...            Riesling   \n",
      "6      start slightly funky strike match aroma open r...     Sauvignon Blanc   \n",
      "7      represent nice value refreshing balanced chard...          Chardonnay   \n",
      "8      preserve lemon lemon curd combine dramatic eff...          Chardonnay   \n",
      "9      hedonistic wine aroma ember dark coffee earth ...               Syrah   \n",
      "10     fruity blackberry cherry flavor easy drink goo...  Cabernet Sauvignon   \n",
      "11     soft ripe perfumed wine fine clean texture app...          Chardonnay   \n",
      "12     much oak wine like inside barrel vanilla butte...  Cabernet Sauvignon   \n",
      "13     bruise apple note scent dry style riesling pal...            Riesling   \n",
      "14     six year age bordeaux blend soft fruity end us...  Cabernet Sauvignon   \n",
      "15     delicious savory little hot alcohol flavor ras...          Pinot Noir   \n",
      "16     always great bottling zaca mesa black bear blo...               Syrah   \n",
      "17     produce acre parcel vine plant sophisticated w...          Chardonnay   \n",
      "18     rich concentrated wine show mountain like arom...  Cabernet Sauvignon   \n",
      "19     note ash cherry plum faint soft feel well pric...  Cabernet Sauvignon   \n",
      "20     entrance smoky aroma ripe intense powerful win...          Pinot Noir   \n",
      "21     solid pinot ripe raspberry cherry fine coastal...          Pinot Noir   \n",
      "22     good balance throughout typifie columbia valle...  Cabernet Sauvignon   \n",
      "23     wine make barrel producer finch hollow offerin...          Chardonnay   \n",
      "24     generic berry cherry aroma along note asphalt ...          Pinot Noir   \n",
      "25     plummy smoky chocolatey aroma lead richly flav...               Syrah   \n",
      "26     romanian chardonnay perfumed nose vanilla whit...          Chardonnay   \n",
      "27     deep stony minerality persist nose finish ligh...            Riesling   \n",
      "28     ripe blackberry aroma carry note prune raisin ...  Cabernet Sauvignon   \n",
      "29     cool climate leyda along coastline sb fairly g...     Sauvignon Blanc   \n",
      "...                                                  ...                 ...   \n",
      "14609  baked pear ripe tropical guava papaya muddle w...          Chardonnay   \n",
      "14610  show richness vineyard deliver huge flamboyant...          Chardonnay   \n",
      "14611  smooth perfume delicious ripe black fruit char...          Pinot Noir   \n",
      "14612  chilly vintage make wine elusive taste spring ...          Pinot Noir   \n",
      "14613  restrained note yellow plum pervade greenish c...     Sauvignon Blanc   \n",
      "14614  strike lime cut rich honey tangerine flavor dr...            Riesling   \n",
      "14615  pungent nose aroma dry herb sweet spice exotic...            Riesling   \n",
      "14616  kogl one great krem vineyard full bodied riesl...            Riesling   \n",
      "14617  ambitious wine aim fence hit double soft super...  Cabernet Sauvignon   \n",
      "14618  zesty tangerine peel apple pear abound crisp l...            Riesling   \n",
      "14619  wine mild lightly fruity front aroma pear appl...          Chardonnay   \n",
      "14620  slightly burn stemmy nose nice chilean pinot b...          Pinot Noir   \n",
      "14621  slick wax lanolin lend mineral tone bright app...            Riesling   \n",
      "14622  wine quite accomplishment flavor complexity gi...  Cabernet Sauvignon   \n",
      "14623  wine reduce dilute astringency bite heel may s...  Cabernet Sauvignon   \n",
      "14624  legacy estate reserve limited selection origin...          Pinot Noir   \n",
      "14625  pungent upfront marry freshly cut grass hint s...     Sauvignon Blanc   \n",
      "14626  simple straightforward cabernet sauvignon nort...  Cabernet Sauvignon   \n",
      "14627  cedar olive accent earthy nose cassis cherry m...  Cabernet Sauvignon   \n",
      "14628  sauvignon blanc make ripe style refresh acidit...     Sauvignon Blanc   \n",
      "14629  aroma similar wine maybe little less forward e...          Chardonnay   \n",
      "14630  wine rich tight texture bring minerality yello...          Chardonnay   \n",
      "14631  first release tranche blue mountain estate fru...  Cabernet Sauvignon   \n",
      "14632  herbal spicy aroma strange like uncooked hot d...          Pinot Noir   \n",
      "14633  riesle become mature foundation perfumed white...            Riesling   \n",
      "14634  fresh fruity wine full red cherry berry flavor...          Pinot Noir   \n",
      "14635  mother vine cabernet drink soft easy gentle ta...  Cabernet Sauvignon   \n",
      "14636  oak chard brim flavor crisp green apple ripe b...          Chardonnay   \n",
      "14637  beautifully balance elegant pinot noir great a...          Pinot Noir   \n",
      "14638  one cellar dry lock hard tannin kind ste gums ...  Cabernet Sauvignon   \n",
      "\n",
      "                Predicted  \n",
      "0      Cabernet Sauvignon  \n",
      "1              Chardonnay  \n",
      "2              Pinot Noir  \n",
      "3      Cabernet Sauvignon  \n",
      "4              Pinot Noir  \n",
      "5                Riesling  \n",
      "6                Riesling  \n",
      "7              Chardonnay  \n",
      "8              Chardonnay  \n",
      "9                   Syrah  \n",
      "10     Cabernet Sauvignon  \n",
      "11             Chardonnay  \n",
      "12     Cabernet Sauvignon  \n",
      "13               Riesling  \n",
      "14             Pinot Noir  \n",
      "15             Pinot Noir  \n",
      "16                  Syrah  \n",
      "17             Chardonnay  \n",
      "18     Cabernet Sauvignon  \n",
      "19     Cabernet Sauvignon  \n",
      "20             Pinot Noir  \n",
      "21             Pinot Noir  \n",
      "22     Cabernet Sauvignon  \n",
      "23             Chardonnay  \n",
      "24             Pinot Noir  \n",
      "25     Cabernet Sauvignon  \n",
      "26             Chardonnay  \n",
      "27               Riesling  \n",
      "28     Cabernet Sauvignon  \n",
      "29        Sauvignon Blanc  \n",
      "...                   ...  \n",
      "14609          Chardonnay  \n",
      "14610          Chardonnay  \n",
      "14611          Pinot Noir  \n",
      "14612          Pinot Noir  \n",
      "14613     Sauvignon Blanc  \n",
      "14614            Riesling  \n",
      "14615            Riesling  \n",
      "14616            Riesling  \n",
      "14617  Cabernet Sauvignon  \n",
      "14618            Riesling  \n",
      "14619          Chardonnay  \n",
      "14620          Pinot Noir  \n",
      "14621            Riesling  \n",
      "14622  Cabernet Sauvignon  \n",
      "14623          Pinot Noir  \n",
      "14624          Pinot Noir  \n",
      "14625     Sauvignon Blanc  \n",
      "14626  Cabernet Sauvignon  \n",
      "14627  Cabernet Sauvignon  \n",
      "14628     Sauvignon Blanc  \n",
      "14629          Chardonnay  \n",
      "14630          Chardonnay  \n",
      "14631  Cabernet Sauvignon  \n",
      "14632          Pinot Noir  \n",
      "14633            Riesling  \n",
      "14634          Pinot Noir  \n",
      "14635  Cabernet Sauvignon  \n",
      "14636          Chardonnay  \n",
      "14637          Pinot Noir  \n",
      "14638  Cabernet Sauvignon  \n",
      "\n",
      "[14639 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "preds = pd.DataFrame({'Review' : test_corpus,\n",
    "                      'Actual' : test_label_names,\n",
    "                      'Predicted' : svm_predictions\n",
    "                      })\n",
    "\n",
    "print(preds)\n",
    "preds.to_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build BOW features on train articles with TFIDF w/ parameters from above\n",
    "tv = TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "                     encoding='utf-8', input='content',\n",
    "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "           ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
    "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
    "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
    "           vocabulary=None)\n",
    "tv_train_features = tv.fit_transform(train_corpus)\n",
    "\n",
    "# transform test articles into features\n",
    "tv_test_features = tv.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(tv.idf_)[::-1]\n",
    "features = tv.get_feature_names()\n",
    "top_n = 25\n",
    "top_features = [features[i] for i in indices[:top_n]]\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
